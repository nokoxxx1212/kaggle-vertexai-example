{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bc3e70-2dcb-4602-b3b6-a2a50defeb4c",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af36aaa-a7bd-4f36-97ce-b3dd09614c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "# -> '20220305052457'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ad767-3147-4035-b4f9-769f1b3f585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\" \n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"\"\n",
    "PIPELINE_ROOT = \"gs://{}\".format(BUCKET_NAME)\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "# -> 'us-central1-aiplatform.googleapis.com'\n",
    "DISPLAY_NAME = \"titanic_\" + TIMESTAMP\n",
    "KAGGLE_COMMIT_MESSAGE = \"update at \" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14478849-f007-4cc8-a133-50f8d34f7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_PATH = \"input/titanic\"\n",
    "TRANSFORMED_DATA_PATH = \"output/titanic/\" + TIMESTAMP\n",
    "TRAINED_MODEL_PATH = \"output/titanic/\" + TIMESTAMP\n",
    "PREDICTED_DATA_PATH = \"output/titanic/\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac08b4a-9540-4039-bc4f-41e6d579c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE_TRANSFORM = REGION + \"-docker.pkg.dev/\" + PROJECT_ID + \"/kaggle/transform:latest\"\n",
    "BASE_IMAGE_TRAINER = REGION + \"-docker.pkg.dev/\" + PROJECT_ID + \"/kaggle/trainer:latest\"\n",
    "BASE_IMAGE_PREDICTOR = REGION + \"-docker.pkg.dev/\" + PROJECT_ID + \"/kaggle/predictor:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2a7fe-3cef-45c2-bd70-206a5dc3d607",
   "metadata": {},
   "source": [
    "## Define components and a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c035b-075e-4a33-a36d-7d9e728b62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Google Cloud project\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f4393-be86-4310-9c7a-25dfcc074527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb56c1e-264f-4947-86c7-c7c3a344b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c8745-5075-496c-929c-eb8f1ab41036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Google Cloud project\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d353c-a16a-4463-a244-395919a3f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE_TRANSFORM)\n",
    "def transform(project_id: str,bucket_name: str, raw_data_path: str, transformed_data_path: str) -> str:\n",
    "    from google.cloud import storage as gcs\n",
    "    from io import BytesIO\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    raw_data_path_train = raw_data_path + '/train.csv'\n",
    "    raw_data_path_test = raw_data_path + '/test.csv'\n",
    "    raw_data_path_gender_submission = raw_data_path + '/gender_submission.csv'\n",
    "    transformed_data_path_y_train = transformed_data_path + \"/y_train.csv\"\n",
    "    transformed_data_path_X_train = transformed_data_path + \"/X_train.csv\"\n",
    "    transformed_data_path_X_test = transformed_data_path + \"/X_test.csv\"\n",
    "    \n",
    "    # input\n",
    "    client = gcs.Client(project_id)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    # train\n",
    "    raw_data_blob_train = bucket.blob(raw_data_path_train)\n",
    "    train = pd.read_csv(BytesIO(raw_data_blob_train.download_as_string()))\n",
    "    # test\n",
    "    raw_data_blob_test = bucket.blob(raw_data_path_test)\n",
    "    test = pd.read_csv(BytesIO(raw_data_blob_test.download_as_string()))\n",
    "    # gender_submission\n",
    "    raw_data_blob_gender_submission = bucket.blob(raw_data_path_gender_submission)\n",
    "    gender_submission = pd.read_csv(BytesIO(raw_data_blob_gender_submission.download_as_string()))\n",
    "\n",
    "    # run\n",
    "    data = pd.concat([train, test], sort=False)\n",
    "\n",
    "    data['Sex'].replace(['male', 'female'], [0, 1], inplace=True)\n",
    "\n",
    "    data['Embarked'].fillna(('S'), inplace=True)\n",
    "    data['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n",
    "\n",
    "    data['Fare'].fillna(np.mean(data['Fare']), inplace=True)\n",
    "\n",
    "    age_avg = data['Age'].mean()\n",
    "    age_std = data['Age'].std()\n",
    "    data['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True)\n",
    "\n",
    "    delete_columns = ['Name', 'PassengerId', 'SibSp', 'Parch', 'Ticket', 'Cabin']\n",
    "    data.drop(delete_columns, axis=1, inplace=True)\n",
    "\n",
    "    train = data[:len(train)]\n",
    "    test = data[len(train):]\n",
    "\n",
    "    y_train = train['Survived']\n",
    "    X_train = train.drop('Survived', axis=1)\n",
    "    X_test = test.drop('Survived', axis=1)\n",
    "    \n",
    "    # output\n",
    "    # y_train\n",
    "    transformed_data_blob_y_train = bucket.blob(transformed_data_path_y_train)\n",
    "    transformed_data_blob_y_train.upload_from_string(y_train.to_csv(sep=\",\"))\n",
    "    # X_train\n",
    "    transformed_data_blob_X_train = bucket.blob(transformed_data_path_X_train)\n",
    "    transformed_data_blob_X_train.upload_from_string(X_train.to_csv(sep=\",\"))\n",
    "    # X_test\n",
    "    transformed_data_blob_X_test = bucket.blob(transformed_data_path_X_test)\n",
    "    transformed_data_blob_X_test.upload_from_string(X_test.to_csv(sep=\",\"))\n",
    "    \n",
    "    return transformed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18694e07-25ae-406e-92b6-65e119b672c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug local\n",
    "transform(PROJECT_ID, BUCKET_NAME, RAW_DATA_PATH, TRANSFORMED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca1044b-c20d-4697-8e03-eb02d47b7f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE_TRAINER)\n",
    "def trainer(project_id: str,bucket_name: str, transformed_data_path: str, trained_model_path: str) -> str:\n",
    "    from google.cloud import storage as gcs\n",
    "    from io import BytesIO\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    transformed_data_path_y_train = transformed_data_path + \"/y_train.csv\"\n",
    "    transformed_data_path_X_train = transformed_data_path + \"/X_train.csv\"\n",
    "    trained_model_path_model = trained_model_path + \"/model_titanic.sav\"\n",
    "\n",
    "    # input\n",
    "    client = gcs.Client(project_id)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    # y_train\n",
    "    transformed_data_blob_y_train = bucket.blob(transformed_data_path_y_train)\n",
    "    y_train = pd.read_csv(BytesIO(transformed_data_blob_y_train.download_as_string()), index_col=0)\n",
    "    # X_train\n",
    "    transformed_data_blob_X_train = bucket.blob(transformed_data_path_X_train)\n",
    "    X_train = pd.read_csv(BytesIO(transformed_data_blob_X_train.download_as_string()), index_col=0)\n",
    "\n",
    "    # run\n",
    "    model = LogisticRegression(penalty='l2', solver='sag', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # output\n",
    "    pickle.dump(model, open('model_titanic.sav', 'wb'))\n",
    "    trained_model_blob_model = bucket.blob(trained_model_path_model)\n",
    "    trained_model_blob_model.upload_from_filename('model_titanic.sav')\n",
    "\n",
    "    return trained_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f343dd-5c82-45f0-a6bf-18b3619091e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug local\n",
    "trainer(PROJECT_ID, BUCKET_NAME, TRANSFORMED_DATA_PATH, TRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c367104-819d-47fc-9498-4d7ff63ae518",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE_PREDICTOR)\n",
    "def predictor(project_id: str,bucket_name: str, raw_data_path: str, transformed_data_path: str, trained_model_path: str, predicted_data_path: str, kaggle_commit_message: str) -> str:\n",
    "    from google.cloud import storage as gcs\n",
    "    from io import BytesIO\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    raw_data_path_gender_submission = raw_data_path + '/gender_submission.csv'\n",
    "    transformed_data_path_X_test = transformed_data_path + \"/X_test.csv\"\n",
    "    trained_model_path_model = trained_model_path + \"/model_titanic.sav\"\n",
    "    predicted_data_path_submission = predicted_data_path + \"/submission.csv\"\n",
    "\n",
    "    # input\n",
    "    client = gcs.Client(project_id)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    # gender_submission\n",
    "    raw_data_blob_gender_submission = bucket.blob(raw_data_path_gender_submission)\n",
    "    gender_submission = pd.read_csv(BytesIO(raw_data_blob_gender_submission.download_as_string()))\n",
    "    # X_test\n",
    "    transformed_data_blob_X_test = bucket.blob(transformed_data_path_X_test)\n",
    "    X_test = pd.read_csv(BytesIO(transformed_data_blob_X_test.download_as_string()), index_col=0)\n",
    "    # model\n",
    "    trained_model_blob = bucket.blob(trained_model_path_model)\n",
    "    trained_model_blob.download_to_filename(\"model_titanic.sav\")\n",
    "    loaded_model = pickle.load(open(\"model_titanic.sav\", \"rb\"))\n",
    "    \n",
    "    # run\n",
    "    y_pred = loaded_model.predict(X_test)\n",
    "    # submit\n",
    "    gender_submission[\"Survived\"] = list(map(int, y_pred))\n",
    "    gender_submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.competition_submit('submission.csv', message=kaggle_commit_message, competition='titanic')\n",
    "    \n",
    "    # output\n",
    "    predicted_data_submission_blob = bucket.blob(predicted_data_path_submission)\n",
    "    predicted_data_submission_blob.upload_from_filename(\"submission.csv\")\n",
    "\n",
    "    return predicted_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfb58f-f8fe-4f47-9768-3697ad84e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug local\n",
    "predictor(PROJECT_ID, BUCKET_NAME, RAW_DATA_PATH, TRANSFORMED_DATA_PATH, TRAINED_MODEL_PATH, PREDICTED_DATA_PATH, KAGGLE_COMMIT_MESSAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da42ad-c050-43a2-b355-76b3b736ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"titanic\",\n",
    "    description=\"pipeline for titanic\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def pipeline(\n",
    "        project_id: str = PROJECT_ID,\n",
    "        bucket_name: str = BUCKET_NAME,\n",
    "        raw_data_path: str = RAW_DATA_PATH,\n",
    "        transformed_data_path: str = TRANSFORMED_DATA_PATH,\n",
    "        trained_model_path: str = TRAINED_MODEL_PATH,\n",
    "        predicted_data_path: str = PREDICTED_DATA_PATH,\n",
    "        kaggle_commit_message: str = KAGGLE_COMMIT_MESSAGE\n",
    "        \n",
    "    ):\n",
    "    transform_task = transform(project_id, bucket_name, raw_data_path, transformed_data_path)\n",
    "    trainer_task = trainer(project_id, bucket_name, transform_task.output, trained_model_path)\n",
    "    predictor_task = predictor(project_id, bucket_name, raw_data_path, transform_task.output, trainer_task.output, predicted_data_path, kaggle_commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c8cd4-46be-48ea-b6e2-31fff44f9ef0",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f805dbe7-72d8-484a-8784-1f91f1351df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler  # noqa: F811\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline_titanic.json\".replace(\" \", \"_\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a22162-10f6-4fa7-bd9b-fd34b58706cd",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa7e1c-9306-40ed-b676-bbd863f62787",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    template_path=\"pipeline_titanic.json\".replace(\" \", \"_\"),\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:latest"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
